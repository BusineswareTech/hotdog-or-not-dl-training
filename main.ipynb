{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This code is based on an assignment from excellent Deep Learning course at https://dlcourse.ai/\n",
    "#\n",
    "import PIL\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, Sampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from onnx import onnx_pb\n",
    "from onnx_coreml import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotdogOrNotDataset(Dataset):\n",
    "# Define our own dataset class. We will load images from files. \n",
    "# The ground truth is calculated based on the filename.\n",
    "#\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        # assumption: the folder contains only normal files, no subfolders\n",
    "        self.filelist = os.listdir(folder)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        filename = self.filelist[index]\n",
    "        img = PIL.Image.open(os.path.join(self.folder, filename))\n",
    "        if (self.transform is not None):\n",
    "            img = self.transform(img)\n",
    "        # We will use more variants of hot dogs, not just classical ones\n",
    "        if filename.startswith(\"chili-dog\") or filename.startswith(\"frankfurter\") or filename.startswith(\"hotdog\"):\n",
    "            y = 1 # This is a hot dog\n",
    "        else: \n",
    "            y = 0 # NOT a hot dog\n",
    "        img_id = filename\n",
    "        \n",
    "        return img, y, img_id\n",
    "\n",
    "# The transformations below are used to distort images, so that model will train better\n",
    "train_dataset = HotdogOrNotDataset(\"train_images/\", \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           transforms.RandomVerticalFlip(),\n",
    "                           transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "                           transforms.RandomRotation(25, resample=PIL.Image.BILINEAR),\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                            \n",
    "                       ]))\n",
    "\n",
    "# No distortion transformations for testing, but we still need to resze image to 224x224 and\n",
    "# adjust the brightness/contrast to standard mean and deviation. \n",
    "test_dataset = HotdogOrNotDataset(\"test_images/\",\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_loaders(train_dataset, split = 0.2, batch_size = 64):\n",
    "# We split the dataset into train part and validation part. We create two data loaders for that.\n",
    "#\n",
    "# Train loader is used to train model.\n",
    "# Validation loader is used to estimate how well we are doing during the training.\n",
    "# Test loader is used AFTER the training is complete to see how well the model was trained.\n",
    "\n",
    "    np.random.seed(0)\n",
    "    data_size = len(train_dataset)\n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    val_split = int(np.floor(split * data_size))\n",
    "    \n",
    "    val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "    num_batches = int((data_size - val_split) / batch_size)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             sampler=val_sampler)\n",
    "    return train_loader, val_loader, num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, loss, optimizer, num_epochs, scheduler=None):\n",
    "# Train model for a specified number of epochs\n",
    "\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    train_loader, val_loader, num_batches = generate_loaders(train_dataset, batch_size=batch_size)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, (x, y, _) in tqdm(enumerate(train_loader), total=len(train_loader)):     \n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)    \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            batch_correct_samples = torch.sum(indices == y_gpu)\n",
    "            correct_samples += batch_correct_samples\n",
    "            total_samples += y.shape[0]\n",
    "            loss_accum += loss_value\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Never use gradient calculations if we don't train the model.\n",
    "        with torch.no_grad():\n",
    "            ave_loss = loss_accum / i\n",
    "            train_accuracy = float(correct_samples) / total_samples\n",
    "            # calculate accuracy on validation dataset\n",
    "            val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "            # keep the history of loss and accuracy in case we'll want to see how the training goes\n",
    "            loss_history.append(float(ave_loss))\n",
    "            train_history.append(train_accuracy)\n",
    "            val_history.append(val_accuracy)\n",
    "        \n",
    "        print('Average loss: %f, Train accuracy: %f, Val accuracy: %f' % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "# Compute accuracy of the model using data from loader    \n",
    "\n",
    "    model.eval() # Evaluation mode\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    for i_step, (x, y, _) in enumerate(loader):\n",
    "        x_gpu = x.to(device)\n",
    "        y_gpu = y.to(device)\n",
    "        # calculate predictions for the batch\n",
    "        prediction = model(x_gpu)\n",
    "        _, indices = torch.max(prediction, 1)\n",
    "        # sum correct predictions\n",
    "        correct_in_batch = torch.sum(indices == y_gpu)\n",
    "        correct_samples += correct_in_batch\n",
    "        total_samples += y.shape[0]\n",
    "\n",
    "    # calculate accuracy across all batches\n",
    "    accuracy = float(correct_samples) / total_samples\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only run on a computer with CUDA GPU\n",
    "device = torch.device('cuda:0')\n",
    "# We will use pre-trained model from Torch model zoo\n",
    "model = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We change the output layer to predict only 2 possible answers: hot dog or not a hot dog\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, 2)\n",
    "\n",
    "# We don't freeze any layers, because we have a small model and a small dataset to train on\n",
    "\n",
    "# Copy model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29adb7bc6b614ee291fa2acc2ed53fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.263595, Train accuracy: 0.879718, Val accuracy: 0.893478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ff9e6c8e0e44008383ea400d96c9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.140342, Train accuracy: 0.942981, Val accuracy: 0.948913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7fd4e76bd5408eafa9ace54689ca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.102030, Train accuracy: 0.961444, Val accuracy: 0.953261\n"
     ]
    }
   ],
   "source": [
    "# Change this to 40 or less if you have 4GB memory on the GPU\n",
    "batch_size = 80\n",
    "# Use standard loss function for classification\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-03)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.25)\n",
    "\n",
    "# 3 epochs are enough for our tiny dataset\n",
    "loss_history, train_history, val_history = train_model(model, train_dataset, loss, \n",
    "                                                       optimizer, 3, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress those pesky warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# store model to file, so we can load it later, if needed\n",
    "torch.save(model, 'my_mobilenet_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file = 'mobilenet_v2-2.onnx'\n",
    "# convert model to ONNX format\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "input_names = ['actual_input_1'] + ['learned_%d' % i for i in range(10)]\n",
    "output_names = ['output1']\n",
    "torch.onnx.export(model, dummy_input, onnx_file, verbose=False, \n",
    "                  input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/151: Converting Node Type Conv\n",
      "2/151: Converting Node Type BatchNormalization\n",
      "3/151: Converting Node Type Clip\n",
      "4/151: Converting Node Type Conv\n",
      "5/151: Converting Node Type BatchNormalization\n",
      "6/151: Converting Node Type Clip\n",
      "7/151: Converting Node Type Conv\n",
      "8/151: Converting Node Type BatchNormalization\n",
      "9/151: Converting Node Type Conv\n",
      "10/151: Converting Node Type BatchNormalization\n",
      "11/151: Converting Node Type Clip\n",
      "12/151: Converting Node Type Conv\n",
      "13/151: Converting Node Type BatchNormalization\n",
      "14/151: Converting Node Type Clip\n",
      "15/151: Converting Node Type Conv\n",
      "16/151: Converting Node Type BatchNormalization\n",
      "17/151: Converting Node Type Conv\n",
      "18/151: Converting Node Type BatchNormalization\n",
      "19/151: Converting Node Type Clip\n",
      "20/151: Converting Node Type Conv\n",
      "21/151: Converting Node Type BatchNormalization\n",
      "22/151: Converting Node Type Clip\n",
      "23/151: Converting Node Type Conv\n",
      "24/151: Converting Node Type BatchNormalization\n",
      "25/151: Converting Node Type Add\n",
      "26/151: Converting Node Type Conv\n",
      "27/151: Converting Node Type BatchNormalization\n",
      "28/151: Converting Node Type Clip\n",
      "29/151: Converting Node Type Conv\n",
      "30/151: Converting Node Type BatchNormalization\n",
      "31/151: Converting Node Type Clip\n",
      "32/151: Converting Node Type Conv\n",
      "33/151: Converting Node Type BatchNormalization\n",
      "34/151: Converting Node Type Conv\n",
      "35/151: Converting Node Type BatchNormalization\n",
      "36/151: Converting Node Type Clip\n",
      "37/151: Converting Node Type Conv\n",
      "38/151: Converting Node Type BatchNormalization\n",
      "39/151: Converting Node Type Clip\n",
      "40/151: Converting Node Type Conv\n",
      "41/151: Converting Node Type BatchNormalization\n",
      "42/151: Converting Node Type Add\n",
      "43/151: Converting Node Type Conv\n",
      "44/151: Converting Node Type BatchNormalization\n",
      "45/151: Converting Node Type Clip\n",
      "46/151: Converting Node Type Conv\n",
      "47/151: Converting Node Type BatchNormalization\n",
      "48/151: Converting Node Type Clip\n",
      "49/151: Converting Node Type Conv\n",
      "50/151: Converting Node Type BatchNormalization\n",
      "51/151: Converting Node Type Add\n",
      "52/151: Converting Node Type Conv\n",
      "53/151: Converting Node Type BatchNormalization\n",
      "54/151: Converting Node Type Clip\n",
      "55/151: Converting Node Type Conv\n",
      "56/151: Converting Node Type BatchNormalization\n",
      "57/151: Converting Node Type Clip\n",
      "58/151: Converting Node Type Conv\n",
      "59/151: Converting Node Type BatchNormalization\n",
      "60/151: Converting Node Type Conv\n",
      "61/151: Converting Node Type BatchNormalization\n",
      "62/151: Converting Node Type Clip\n",
      "63/151: Converting Node Type Conv\n",
      "64/151: Converting Node Type BatchNormalization\n",
      "65/151: Converting Node Type Clip\n",
      "66/151: Converting Node Type Conv\n",
      "67/151: Converting Node Type BatchNormalization\n",
      "68/151: Converting Node Type Add\n",
      "69/151: Converting Node Type Conv\n",
      "70/151: Converting Node Type BatchNormalization\n",
      "71/151: Converting Node Type Clip\n",
      "72/151: Converting Node Type Conv\n",
      "73/151: Converting Node Type BatchNormalization\n",
      "74/151: Converting Node Type Clip\n",
      "75/151: Converting Node Type Conv\n",
      "76/151: Converting Node Type BatchNormalization\n",
      "77/151: Converting Node Type Add\n",
      "78/151: Converting Node Type Conv\n",
      "79/151: Converting Node Type BatchNormalization\n",
      "80/151: Converting Node Type Clip\n",
      "81/151: Converting Node Type Conv\n",
      "82/151: Converting Node Type BatchNormalization\n",
      "83/151: Converting Node Type Clip\n",
      "84/151: Converting Node Type Conv\n",
      "85/151: Converting Node Type BatchNormalization\n",
      "86/151: Converting Node Type Add\n",
      "87/151: Converting Node Type Conv\n",
      "88/151: Converting Node Type BatchNormalization\n",
      "89/151: Converting Node Type Clip\n",
      "90/151: Converting Node Type Conv\n",
      "91/151: Converting Node Type BatchNormalization\n",
      "92/151: Converting Node Type Clip\n",
      "93/151: Converting Node Type Conv\n",
      "94/151: Converting Node Type BatchNormalization\n",
      "95/151: Converting Node Type Conv\n",
      "96/151: Converting Node Type BatchNormalization\n",
      "97/151: Converting Node Type Clip\n",
      "98/151: Converting Node Type Conv\n",
      "99/151: Converting Node Type BatchNormalization\n",
      "100/151: Converting Node Type Clip\n",
      "101/151: Converting Node Type Conv\n",
      "102/151: Converting Node Type BatchNormalization\n",
      "103/151: Converting Node Type Add\n",
      "104/151: Converting Node Type Conv\n",
      "105/151: Converting Node Type BatchNormalization\n",
      "106/151: Converting Node Type Clip\n",
      "107/151: Converting Node Type Conv\n",
      "108/151: Converting Node Type BatchNormalization\n",
      "109/151: Converting Node Type Clip\n",
      "110/151: Converting Node Type Conv\n",
      "111/151: Converting Node Type BatchNormalization\n",
      "112/151: Converting Node Type Add\n",
      "113/151: Converting Node Type Conv\n",
      "114/151: Converting Node Type BatchNormalization\n",
      "115/151: Converting Node Type Clip\n",
      "116/151: Converting Node Type Conv\n",
      "117/151: Converting Node Type BatchNormalization\n",
      "118/151: Converting Node Type Clip\n",
      "119/151: Converting Node Type Conv\n",
      "120/151: Converting Node Type BatchNormalization\n",
      "121/151: Converting Node Type Conv\n",
      "122/151: Converting Node Type BatchNormalization\n",
      "123/151: Converting Node Type Clip\n",
      "124/151: Converting Node Type Conv\n",
      "125/151: Converting Node Type BatchNormalization\n",
      "126/151: Converting Node Type Clip\n",
      "127/151: Converting Node Type Conv\n",
      "128/151: Converting Node Type BatchNormalization\n",
      "129/151: Converting Node Type Add\n",
      "130/151: Converting Node Type Conv\n",
      "131/151: Converting Node Type BatchNormalization\n",
      "132/151: Converting Node Type Clip\n",
      "133/151: Converting Node Type Conv\n",
      "134/151: Converting Node Type BatchNormalization\n",
      "135/151: Converting Node Type Clip\n",
      "136/151: Converting Node Type Conv\n",
      "137/151: Converting Node Type BatchNormalization\n",
      "138/151: Converting Node Type Add\n",
      "139/151: Converting Node Type Conv\n",
      "140/151: Converting Node Type BatchNormalization\n",
      "141/151: Converting Node Type Clip\n",
      "142/151: Converting Node Type Conv\n",
      "143/151: Converting Node Type BatchNormalization\n",
      "144/151: Converting Node Type Clip\n",
      "145/151: Converting Node Type Conv\n",
      "146/151: Converting Node Type BatchNormalization\n",
      "147/151: Converting Node Type Conv\n",
      "148/151: Converting Node Type BatchNormalization\n",
      "149/151: Converting Node Type Clip\n",
      "150/151: Converting Node Type ReduceMean\n",
      "151/151: Converting Node Type Gemm\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "# convert ONNX to CoreML, so it can run on iOS\n",
    "model_file = open(onnx_file, 'rb')\n",
    "model_proto = onnx_pb.ModelProto()\n",
    "model_proto.ParseFromString(model_file.read())\n",
    "coreml_model = convert(model_proto, image_input_names=['actual_input_1'], image_output_names=['outputImage'])\n",
    "coreml_model.save('coreml_output-2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
